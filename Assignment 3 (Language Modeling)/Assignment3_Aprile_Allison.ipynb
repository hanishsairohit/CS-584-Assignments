{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3_Aprile_Allison.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPVzpA9j5EE4",
        "outputId": "f97a89c3-c0c6-44d4-f0fa-bffb1ba612f3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from google.colab import drive\n",
        "from collections import defaultdict, Counter\n",
        "from nltk.util import pad_sequence, ngrams, bigrams\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc0ccPkm6FWO",
        "outputId": "1c1d67f5-38e7-4311-a335-64dd54848801"
      },
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2Dr8kIV2P_f"
      },
      "source": [
        "# **1. N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a50H2RT5m8Y"
      },
      "source": [
        "## <font color=navy> **Preprocessing (1a)**\n",
        "* Appears that sentences are divided by newline characters, so will split text into list of sentences; will use those to determine < s > and < /s > placement\n",
        "\n",
        "* Text contains < unk > token; because one of N-gram model's issue is not being able to predict words not in vocabulary, I will keep this token\n",
        "\n",
        "* Other preprocessing will just include normalizing (converting to all lower case)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SGS2FH96WP8"
      },
      "source": [
        "# Data processing function\n",
        "def process_txt(file):\n",
        "  with open('/content/drive/My Drive/' + file) as f:\n",
        "    # Read as string\n",
        "    text = f.read()\n",
        "\n",
        "    # Normalize text by making all lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Split into sentences\n",
        "    sentences = text.split('\\n')\n",
        "\n",
        "    # Add <s> and </s> tokens\n",
        "    sentences = ['<s>' + s + '</s>' for s in sentences]\n",
        "\n",
        "    # Return  sentences\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hb902v87T57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "750fa530-f586-4379-f868-d54ef27d7ec9"
      },
      "source": [
        "# Preprocess training and validation sets\n",
        "train = process_txt('train.5k.txt')\n",
        "validation = process_txt('valid.txt')\n",
        "\n",
        "# Confirm \n",
        "print(train[0])\n",
        "print(validation[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter </s>\n",
            "<s> consumers may want to move their telephones a little closer to the tv set </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifSUvBFIL9Cn"
      },
      "source": [
        "Next, the sentences are tokenized. The tokenized lists are also used to create the training vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTDoO5ziJ8Lh"
      },
      "source": [
        "# Tokenize training sentences\n",
        "train_tokenized = [sent.split(' ') for sent in train]\n",
        "\n",
        "# Create vocabulary - join tokenized lists and then take set\n",
        "all_tokens = []\n",
        "for t in train_tokenized:\n",
        "  all_tokens += t\n",
        "\n",
        "vocabulary = list(set(all_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl68B0cBgCDX"
      },
      "source": [
        "# Get unigram counts\n",
        "unigrams = dict(Counter(all_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pogfn-SsCjfN"
      },
      "source": [
        "# Process input.txt\n",
        "with open('/content/drive/My Drive/' + 'input.txt') as f:\n",
        "    # Read as string\n",
        "    input_text = f.read()\n",
        "\n",
        "    # Get sequences\n",
        "    input_sequences = input_text.split('\\n')\n",
        "\n",
        "    # Remove end line\n",
        "    input_sequences = [' '.join(s.split(' ')[:-1]) for s in input_sequences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn7aEeuODCRb"
      },
      "source": [
        "# Get first 30 lines of input sequences\n",
        "input_sequences_30 = input_sequences[:30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG7Qj3XmbmQN"
      },
      "source": [
        "## <font color=navy> **N-gram model (1b)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1CwuOW4biTP"
      },
      "source": [
        "# N-gram model\n",
        "def train_ngram(sentences, n_val=2):\n",
        "  # Define dictionary to hold probabilities, and initialize nested count values to 0\n",
        "  counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "  ngram = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "  # Get count values (for Maximum Likelihood Estimate of conditional probabilities)\n",
        "  if n_val == 3: # For trigram\n",
        "    for s in sentences:\n",
        "      print(s)\n",
        "      for w0, w1, w2 in ngrams(s, n=3): # Unpack words\n",
        "        # Update count (bigram prior > unigram)\n",
        "        counts[(w0, w1)][w2] += 1 \n",
        "        ngram[(w0, w1)][w2] += 1\n",
        "\n",
        "  else: # For bigram or any other n value (error handling)\n",
        "    for s in sentences:\n",
        "      for w0, w1 in ngrams(s, n=2): # Unpack words\n",
        "        # Update count (unigram prior > unigram)\n",
        "        counts[w0][w1] += 1 \n",
        "        ngram[w0][w1] += 1\n",
        "\n",
        "  # Get probabilities \n",
        "  # Normalize numerator counts (either bigram or unigram - prior) by unigrams\n",
        "  for prior in ngram:\n",
        "    # Get unigram (denominator) total\n",
        "    count_unigram = sum(ngram[prior].values())\n",
        "\n",
        "    # Normalize each value (divide by unigram total)\n",
        "    for u in ngram[prior]:\n",
        "        ngram[prior][u] /= float(count_unigram)\n",
        "\n",
        "  # Return mle estimation and counts (used for later smoothing)\n",
        "  return pd.DataFrame.from_dict(ngram).fillna(0), pd.DataFrame.from_dict(counts).fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvOzBZxgUf1e"
      },
      "source": [
        "# Get ngram model and counts\n",
        "mle, mle_counts = train_ngram(train_tokenized, n_val=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "wT5lGwfqVIiD",
        "outputId": "1b68db65-7132-47aa-d7f4-b787d66587ae"
      },
      "source": [
        "mle"
      ],
      "execution_count": 512,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>&lt;s&gt;</th>\n",
              "      <th>aer</th>\n",
              "      <th>banknote</th>\n",
              "      <th>berlitz</th>\n",
              "      <th>calloway</th>\n",
              "      <th>centrust</th>\n",
              "      <th>cluett</th>\n",
              "      <th>fromstein</th>\n",
              "      <th>gitano</th>\n",
              "      <th>guterman</th>\n",
              "      <th>hydro-quebec</th>\n",
              "      <th>ipo</th>\n",
              "      <th>kia</th>\n",
              "      <th>memotec</th>\n",
              "      <th>mlx</th>\n",
              "      <th>nahb</th>\n",
              "      <th>punts</th>\n",
              "      <th>rake</th>\n",
              "      <th>regatta</th>\n",
              "      <th>rubens</th>\n",
              "      <th>sim</th>\n",
              "      <th>snack-food</th>\n",
              "      <th>ssangyong</th>\n",
              "      <th>swapo</th>\n",
              "      <th>wachter</th>\n",
              "      <th>pierre</th>\n",
              "      <th>&lt;unk&gt;</th>\n",
              "      <th>&lt;num&gt;</th>\n",
              "      <th>years</th>\n",
              "      <th>old</th>\n",
              "      <th>will</th>\n",
              "      <th>join</th>\n",
              "      <th>the</th>\n",
              "      <th>board</th>\n",
              "      <th>as</th>\n",
              "      <th>a</th>\n",
              "      <th>nonexecutive</th>\n",
              "      <th>director</th>\n",
              "      <th>nov.</th>\n",
              "      <th>mr.</th>\n",
              "      <th>...</th>\n",
              "      <th>dealership</th>\n",
              "      <th>unrelated</th>\n",
              "      <th>laurel</th>\n",
              "      <th>intervene</th>\n",
              "      <th>adversary</th>\n",
              "      <th>inclined</th>\n",
              "      <th>reconsider</th>\n",
              "      <th>unfortunate</th>\n",
              "      <th>onerous</th>\n",
              "      <th>defend</th>\n",
              "      <th>brick</th>\n",
              "      <th>pre-trial</th>\n",
              "      <th>intact</th>\n",
              "      <th>innocent</th>\n",
              "      <th>comic</th>\n",
              "      <th>sand</th>\n",
              "      <th>await</th>\n",
              "      <th>charitable</th>\n",
              "      <th>whatever</th>\n",
              "      <th>newsprint</th>\n",
              "      <th>excellent</th>\n",
              "      <th>beatrice</th>\n",
              "      <th>scaled</th>\n",
              "      <th>reset</th>\n",
              "      <th>float</th>\n",
              "      <th>wastewater</th>\n",
              "      <th>uninsured</th>\n",
              "      <th>remic</th>\n",
              "      <th>20-year</th>\n",
              "      <th>weighted</th>\n",
              "      <th>j.c.</th>\n",
              "      <th>penney</th>\n",
              "      <th>railway</th>\n",
              "      <th>monte</th>\n",
              "      <th>di</th>\n",
              "      <th>deutsche</th>\n",
              "      <th>owe</th>\n",
              "      <th>minpeco</th>\n",
              "      <th>minerals</th>\n",
              "      <th>abramson</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>aer</th>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pierre</th>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mr.</th>\n",
              "      <td>0.0320</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.001035</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003676</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rudolph</th>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>0.0228</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.021515</td>\n",
              "      <td>0.024586</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011765</td>\n",
              "      <td>0.147059</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>a.g.</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s&amp;ls</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unexpectedly</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>proud</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>penney</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7185 rows × 7185 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 <s>  aer  banknote  berlitz  ...  owe  minpeco  minerals  abramson\n",
              "aer           0.0002  0.0       0.0      0.0  ...  0.0      0.0       0.0       0.0\n",
              "pierre        0.0002  0.0       0.0      0.0  ...  0.0      0.0       0.0       0.0\n",
              "mr.           0.0320  0.0       0.0      0.0  ...  0.0      0.0       0.0       0.0\n",
              "rudolph       0.0002  0.0       0.0      0.0  ...  0.0      0.0       0.0       0.0\n",
              "a             0.0228  0.0       0.0      0.0  ...  0.0      0.0       0.0       0.0\n",
              "...              ...  ...       ...      ...  ...  ...      ...       ...       ...\n",
              "a.g.          0.0000  0.0       0.0      0.0  ...  0.0      0.0       0.0       0.0\n",
              "s&ls          0.0000  0.0       0.0      0.0  ...  0.0      0.0       0.0       0.0\n",
              "unexpectedly  0.0000  0.0       0.0      0.0  ...  0.0      0.0       0.0       0.0\n",
              "proud         0.0000  0.0       0.0      0.0  ...  0.0      0.0       0.0       0.0\n",
              "penney        0.0000  0.0       0.0      0.0  ...  0.0      0.0       0.0       0.0\n",
              "\n",
              "[7185 rows x 7185 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 512
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veYqeC5dBqPO"
      },
      "source": [
        "## <font color=navy> **Good Turing smoothing (1c)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2DFAeWGfXzb"
      },
      "source": [
        "def train_gt_ngram(count):\n",
        "  # Get count as an array\n",
        "  gt_count = count.copy()\n",
        "  count_array = gt_count.values\n",
        "\n",
        "  # Define c* dictionary\n",
        "  adjusted_count = count_array.copy()\n",
        "\n",
        "  # Get counts of counts in count_array\n",
        "  val, cnt = np.unique(count_array, return_counts=True)\n",
        "  val = list(val)\n",
        "  cnt = list(cnt)\n",
        "\n",
        "  # Get N value (for Pgt denominator)\n",
        "  N =sum(val[j]*cnt[j] for j in list(range(len(val))))\n",
        "\n",
        "  # For every count c, compute adjusted count c*\n",
        "  for i in [0,1]:#range(len(val) - 1):\n",
        "    # Compute adjusted count c*\n",
        "    c_star = (val[i] + 1)*cnt[i+1] / cnt[i]\n",
        "\n",
        "    # Replace in adjusted_count\n",
        "    adjusted_count = np.where(adjusted_count == val[i], c_star, adjusted_count)\n",
        "  \n",
        "  # Replace count values with adjusted counts\n",
        "  gt_count[:] = adjusted_count\n",
        "\n",
        "  # Get probability Pgt\n",
        "  pgt = gt_count.copy()\n",
        "  pgt.values /= N\n",
        "\n",
        "  # Return Good Turing estimation and adjusted counts\n",
        "  return pgt, gt_count    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KBIuCPuJgRg"
      },
      "source": [
        "# Get GT estimation and counts\n",
        "pgt, gt_counts = train_gt_ngram(mle_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "FexT2yWUVK_D",
        "outputId": "8783f463-7016-4229-f2ce-48b36ba97306"
      },
      "source": [
        "pgt"
      ],
      "execution_count": 513,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>&lt;s&gt;</th>\n",
              "      <th>aer</th>\n",
              "      <th>banknote</th>\n",
              "      <th>berlitz</th>\n",
              "      <th>calloway</th>\n",
              "      <th>centrust</th>\n",
              "      <th>cluett</th>\n",
              "      <th>fromstein</th>\n",
              "      <th>gitano</th>\n",
              "      <th>guterman</th>\n",
              "      <th>hydro-quebec</th>\n",
              "      <th>ipo</th>\n",
              "      <th>kia</th>\n",
              "      <th>memotec</th>\n",
              "      <th>mlx</th>\n",
              "      <th>nahb</th>\n",
              "      <th>punts</th>\n",
              "      <th>rake</th>\n",
              "      <th>regatta</th>\n",
              "      <th>rubens</th>\n",
              "      <th>sim</th>\n",
              "      <th>snack-food</th>\n",
              "      <th>ssangyong</th>\n",
              "      <th>swapo</th>\n",
              "      <th>wachter</th>\n",
              "      <th>pierre</th>\n",
              "      <th>&lt;unk&gt;</th>\n",
              "      <th>&lt;num&gt;</th>\n",
              "      <th>years</th>\n",
              "      <th>old</th>\n",
              "      <th>will</th>\n",
              "      <th>join</th>\n",
              "      <th>the</th>\n",
              "      <th>board</th>\n",
              "      <th>as</th>\n",
              "      <th>a</th>\n",
              "      <th>nonexecutive</th>\n",
              "      <th>director</th>\n",
              "      <th>nov.</th>\n",
              "      <th>mr.</th>\n",
              "      <th>...</th>\n",
              "      <th>dealership</th>\n",
              "      <th>unrelated</th>\n",
              "      <th>laurel</th>\n",
              "      <th>intervene</th>\n",
              "      <th>adversary</th>\n",
              "      <th>inclined</th>\n",
              "      <th>reconsider</th>\n",
              "      <th>unfortunate</th>\n",
              "      <th>onerous</th>\n",
              "      <th>defend</th>\n",
              "      <th>brick</th>\n",
              "      <th>pre-trial</th>\n",
              "      <th>intact</th>\n",
              "      <th>innocent</th>\n",
              "      <th>comic</th>\n",
              "      <th>sand</th>\n",
              "      <th>await</th>\n",
              "      <th>charitable</th>\n",
              "      <th>whatever</th>\n",
              "      <th>newsprint</th>\n",
              "      <th>excellent</th>\n",
              "      <th>beatrice</th>\n",
              "      <th>scaled</th>\n",
              "      <th>reset</th>\n",
              "      <th>float</th>\n",
              "      <th>wastewater</th>\n",
              "      <th>uninsured</th>\n",
              "      <th>remic</th>\n",
              "      <th>20-year</th>\n",
              "      <th>weighted</th>\n",
              "      <th>j.c.</th>\n",
              "      <th>penney</th>\n",
              "      <th>railway</th>\n",
              "      <th>monte</th>\n",
              "      <th>di</th>\n",
              "      <th>deutsche</th>\n",
              "      <th>owe</th>\n",
              "      <th>minpeco</th>\n",
              "      <th>minerals</th>\n",
              "      <th>abramson</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>aer</th>\n",
              "      <td>2.951658e-06</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pierre</th>\n",
              "      <td>2.951658e-06</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mr.</th>\n",
              "      <td>1.442429e-03</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>1.983340e-04</td>\n",
              "      <td>3.606073e-05</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>1.803036e-05</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rudolph</th>\n",
              "      <td>2.951658e-06</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>1.027731e-03</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>1.090837e-03</td>\n",
              "      <td>8.564422e-04</td>\n",
              "      <td>1.803036e-05</td>\n",
              "      <td>2.951658e-06</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>2.951658e-06</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>2.951658e-06</td>\n",
              "      <td>7.212145e-04</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>a.g.</th>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s&amp;ls</th>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unexpectedly</th>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>proud</th>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>penney</th>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>2.951658e-06</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "      <td>7.190984e-09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7185 rows × 7185 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                       <s>           aer  ...      minerals      abramson\n",
              "aer           2.951658e-06  7.190984e-09  ...  7.190984e-09  7.190984e-09\n",
              "pierre        2.951658e-06  7.190984e-09  ...  7.190984e-09  7.190984e-09\n",
              "mr.           1.442429e-03  7.190984e-09  ...  7.190984e-09  7.190984e-09\n",
              "rudolph       2.951658e-06  7.190984e-09  ...  7.190984e-09  7.190984e-09\n",
              "a             1.027731e-03  7.190984e-09  ...  7.190984e-09  7.190984e-09\n",
              "...                    ...           ...  ...           ...           ...\n",
              "a.g.          7.190984e-09  7.190984e-09  ...  7.190984e-09  7.190984e-09\n",
              "s&ls          7.190984e-09  7.190984e-09  ...  7.190984e-09  7.190984e-09\n",
              "unexpectedly  7.190984e-09  7.190984e-09  ...  7.190984e-09  7.190984e-09\n",
              "proud         7.190984e-09  7.190984e-09  ...  7.190984e-09  7.190984e-09\n",
              "penney        7.190984e-09  7.190984e-09  ...  7.190984e-09  7.190984e-09\n",
              "\n",
              "[7185 rows x 7185 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 513
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHd9QwDGZ0Kw"
      },
      "source": [
        "## <font color=navy> **Kneser-Ney smoothing (1d)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-xDmktGSVNH"
      },
      "source": [
        "# Specific to bigram\n",
        "def train_kn_ngram(counts, unigram, d=0.75):\n",
        "  # Make copy of counts for probability\n",
        "  pkn = counts.copy()\n",
        "\n",
        "  # Calculate number of unique bigrams\n",
        "  num_unique_bigrams = sum((counts != 0).sum().values)\n",
        "\n",
        "  # Iterate through every w_i-1 and wi pair\n",
        "  i = 1 # For tracking training progress\n",
        "\n",
        "  for prior in list(counts.columns):\n",
        "    # Print progress\n",
        "    sys.stdout.flush()\n",
        "    sys.stdout.write('\\rEvaluating prior (w_i-1) %d out of %d' % (i, pkn.shape[1]))\n",
        "\n",
        "    for wi in list(counts.index):\n",
        "      # Calculate bigram probability\n",
        "      bigram_prob = max(counts.loc[wi, prior] - d, 0) / unigram[prior]\n",
        "\n",
        "      # Calculate normalizing constant (discounted probability mass)\n",
        "      lambda_prior = (d / unigram[prior]) * (counts.loc[:, prior] != 0).sum()\n",
        "\n",
        "      # Calculate p(continuation) \n",
        "      # = number of words that wi follows / number of unique bigrams\n",
        "      prob_continuation = (counts.loc[wi, :] != 0).sum() / num_unique_bigrams\n",
        "\n",
        "      # Calculate pkn(wi|prior) and store in pkn DataFrame\n",
        "      pkn.loc[wi, prior] = bigram_prob + (lambda_prior * prob_continuation)\n",
        "\n",
        "    i += 1\n",
        "  # Return Kneser-Ney probability estimation\n",
        "  return pkn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEMxsXQDGhPR"
      },
      "source": [
        "<font color='maroon'>**Note:** Because of my nested for loop, this operation was expected to take 9 hours. I tried to thread it earlier, but Google Colab kept throwing RAM overload errors. It does run and the computation does seem correct, but unfortunately it was taking too long. In the future, I would eliminate the for loop and try to incorproate either a smaller vocabulary or vectorize the code more (i.e. use matrix operations)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "EySOU0type-V",
        "outputId": "409400c5-c8fe-4ebf-d23d-d1d451ae5f12"
      },
      "source": [
        "# Get PKN estimates\n",
        "pkn = train_kn_ngram(mle_counts, unigrams)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating prior (w_i-1) 27 out of 7185"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-476-a0ec3dfc4479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get PKN estimates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpkn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_kn_ngram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmle_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munigrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-113-400e4c623a4a>\u001b[0m in \u001b[0;36mtrain_kn_ngram\u001b[0;34m(counts, unigram, d)\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0;31m# Calculate p(continuation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0;31m# = number of words that wi follows / number of unique bigrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0mprob_continuation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_unique_bigrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;31m# Calculate pkn(wi|prior) and store in pkn DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    871\u001b[0m                     \u001b[0;31m# AttributeError for IntervalTree get_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    784\u001b[0m                 \u001b[0;31m# We don't need to check for tuples here because those are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0;31m#  caught by the _is_nested_tuple_indexer check above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m                 \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0;31m# We should never have a scalar section here, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;31m# GH#5667 this will fail if the label is not present in the axis.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_lowerdim_multi_index_axis0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3517\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3518\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3519\u001b[0;31m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3520\u001b[0m             )\n\u001b[1;32m   3521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mfrom_array\u001b[0;34m(cls, array, index)\u001b[0m\n\u001b[1;32m   1576\u001b[0m         \u001b[0mConstructor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0myet\u001b[0m \u001b[0ma\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m         \"\"\"\n\u001b[0;32m-> 1578\u001b[0;31m         \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1579\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[1;32m   2742\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatetimeArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2744\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_ndim\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             raise ValueError(\n\u001b[1;32m    131\u001b[0m                 \u001b[0;34mf\"Wrong number of items passed {len(self.values)}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGebINPSHg1k"
      },
      "source": [
        "## <font color=navy> **Predict next word in valid set (using sliding window) (1e)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6hZPY2XIZAd"
      },
      "source": [
        "# Prediction function (Bigram)\n",
        "def ngram_predict(sequence, prob):\n",
        "  # Get last word (w_i-1)\n",
        "  prior = sequence.split(' ')[-1]\n",
        "\n",
        "  # If prior is not vocabulary, get maximum probability based on '<unk>'\n",
        "  if prior not in vocabulary:\n",
        "    prior = '<unk>'\n",
        "\n",
        "  # Get index of maximum probability and return (since it is a word)\n",
        "  # Define wi probability values\n",
        "  wi_probs = list(prob.loc[:, prior].values)\n",
        "\n",
        "  # Since there could be ties, index returns first occurence\n",
        "  return prob.index[wi_probs.index(max(wi_probs))]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGVDxVT7Kd6l",
        "outputId": "c1115e04-43be-40e3-c362-722e14b21d3b"
      },
      "source": [
        "# For validation set, remove '</s>' token and also last word. The last word will be used for validation\n",
        "test_clean = [v[4:-5] for v in validation]\n",
        "\n",
        "# Create X sequence and y (last word in sequence)\n",
        "X_test = [' '.join(x.split(' ')[:-1]) for x in test_clean]\n",
        "y_test = [x.split(' ')[-1] for x in test_clean]\n",
        "\n",
        "# Confirm processing\n",
        "print(test_clean[0])\n",
        "print(X_test[0])\n",
        "print(y_test[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "consumers may want to move their telephones a little closer to the tv set\n",
            "consumers may want to move their telephones a little closer to the tv\n",
            "set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "JRAJqsFsOedC",
        "outputId": "e6d8dc13-4e3a-45cb-8f0b-7cf7f5d0cc4e"
      },
      "source": [
        "# Get predictions - MLE\n",
        "mle_test_pred = [ngram_predict(X_test[i], mle) for i in [0, 1, 2, 3, 4]]\n",
        "\n",
        "# Get predictions - Good Turing\n",
        "good_turing_test_pred = [ngram_predict(X_test[i], pgt) for i in [0, 1, 2, 3, 4, 5]] # For some reason, list(range(len(X_test))) is failing\n",
        "\n",
        "'''\n",
        "# Get predictions - Kneser-Ney Smoothing\n",
        "kn_test_pred = [ngram_preidct(X_test[i], pkn) for i in list(range(len(X_test)))]\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Get predictions - Kneser-Ney Smoothing\\nkn_test_pred = [ngram_preidct(X_test[i], pkn) for i in list(range(len(X_test)))]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 506
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsEiLvX5QjdS"
      },
      "source": [
        "# Perplexity (Bigram)\n",
        "def bigram_perplexity(prob, test_sequences, ground_truths):\n",
        "  # Get list of priors\n",
        "  priors = []\n",
        "\n",
        "  # Add priors to list\n",
        "  for sequence in test_sequences:\n",
        "    # Get lislast word (w_i-1)\n",
        "    prior = sequence.split(' ')[-1]\n",
        "\n",
        "    # If prior is not vocabulary, get maximum probability based on '<unk>'\n",
        "    if prior not in list(prob.columns):\n",
        "      prior = '<unk>'\n",
        "\n",
        "  priors.append(prior)\n",
        "\n",
        "  # Take the product of all the inverse correct probabilities of (wi|wi_1)\n",
        "  prod = 1\n",
        "\n",
        "  for gt, p in list(zip(ground_truths, priors)):\n",
        "    # Set ground truth value as '<unk>' if not in vocabulary\n",
        "    if gt not in list(prob.index):\n",
        "      gt = '<unk>'\n",
        "\n",
        "    # Multipy (1/P(wi|w_i-1))\n",
        "    prod *= (1 / prob.loc[gt, p])\n",
        "\n",
        "  # Return the product to the 1/N power\n",
        "  return prod**(1/len(priors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "n8i-etb7T5UC",
        "outputId": "8d49bf0c-82a1-4e80-dc34-94e13418e6db"
      },
      "source": [
        "# Report perplexity scores on test set\n",
        "print('MLE: ', bigram_perplexity(mle, X_test, y_test))\n",
        "print('Good Turing: ', bigram_perplexity(pgt, X_test, y_test))\n",
        "\n",
        "'''\n",
        "print('Kneser-Ney: ', bigram_perplexity(pkn, X_test, y_test))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLE:  2812.0\n",
            "Good Turing:  55462.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nprint('Kneser-Ney: ', bigram_perplexity(pkn, X_test, y_test))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 511
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMb_KjlTUaPt"
      },
      "source": [
        "Because the goal is to minimize the perplexity, it is not ideal that these perplexity scores are so high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyHKyWHxHtvE"
      },
      "source": [
        "## <font color=navy> **Predict input_sequence next word (1f)**\n",
        "Because I could not train the Kneser-Key N-gram model, I outputed the results for both the MLE and Good Turning estimates on the input sequence prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvVTRduAHdQW",
        "outputId": "7318742b-dc12-4af6-ef65-1f750aaddcaa"
      },
      "source": [
        "# Bigram MLE - Print the 30 sequences and their next words (specified in ** **)\n",
        "print('MLE (BIGRAM) PREDICTIONS\\n')\n",
        "for i in range(30):\n",
        "  # Get prediction\n",
        "  pred = ngram_predict(input_sequences_30[i], mle)\n",
        "\n",
        "  # Print completed sequence\n",
        "  print(input_sequences_30[i] + ' ' + '**' + pred + '**')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLE (BIGRAM) PREDICTIONS\n",
            "\n",
            "but while the new york stock exchange did n't fall **in**\n",
            "some circuit breakers installed after the october N crash failed **to**\n",
            "the N stock specialist firms on the big board floor **traders**\n",
            "big investment banks refused to step up to the plate **a**\n",
            "heavy selling of standard & poor 's 500-stock index futures **prices**\n",
            "seven big board stocks ual amr bankamerica walt disney capital **markets**\n",
            "once again the specialists were not able to handle the **<unk>**\n",
            "<unk> james <unk> chairman of specialists henderson brothers inc. it **is**\n",
            "when the dollar is in a <unk> even central banks **</s>**\n",
            "speculators are calling for a degree of liquidity that is **a**\n",
            "many money managers and some traders had already left their **<unk>**\n",
            "then in a <unk> plunge the dow jones industrials in **the**\n",
            "<unk> trading accelerated to N million shares a record for **the**\n",
            "at the end of the day N million shares were **n't**\n",
            "the dow 's decline was second in point terms only **<num>**\n",
            "in percentage terms however the dow 's dive was the **<unk>**\n",
            "shares of ual the parent of united airlines were extremely **high**\n",
            "wall street 's takeover-stock speculators or risk arbitragers had placed **convertible**\n",
            "at N p.m. edt came the <unk> news the big **board**\n",
            "on the exchange floor as soon as ual stopped trading **</s>**\n",
            "several traders could be seen shaking their heads when the **<unk>**\n",
            "for weeks the market had been nervous about takeovers after **the**\n",
            "and N minutes after the ual trading halt came news **that**\n",
            "arbitragers could n't dump their ual stock but they rid **<unk>**\n",
            "for example their selling caused trading halts to be declared **a**\n",
            "but as panic spread speculators began to sell blue-chip stocks **</s>**\n",
            "when trading was halted in philip morris the stock was **<unk>**\n",
            "selling <unk> because of waves of automatic stop-loss orders which **has**\n",
            "most of the stock selling pressure came from wall street **'s**\n",
            "traders said most of their major institutional investors on the **<unk>**\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOQhjscNLj59",
        "outputId": "90fa0c8e-e720-4062-a5e9-0db995bc4478"
      },
      "source": [
        "# Good Turing - Print the 30 sequences and their next words (specified in ** **)\n",
        "print('GOOD TURING PREDICTIONS\\n')\n",
        "for i in range(30):\n",
        "  # Get prediction\n",
        "  pred = ngram_predict(input_sequences_30[i], pgt)\n",
        "\n",
        "  # Print completed sequence\n",
        "  print(input_sequences_30[i] + ' ' + '**' + pred + '**')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GOOD TURING PREDICTIONS\n",
            "\n",
            "but while the new york stock exchange did n't fall **in**\n",
            "some circuit breakers installed after the october N crash failed **to**\n",
            "the N stock specialist firms on the big board floor **traders**\n",
            "big investment banks refused to step up to the plate **a**\n",
            "heavy selling of standard & poor 's 500-stock index futures **prices**\n",
            "seven big board stocks ual amr bankamerica walt disney capital **markets**\n",
            "once again the specialists were not able to handle the **<unk>**\n",
            "<unk> james <unk> chairman of specialists henderson brothers inc. it **is**\n",
            "when the dollar is in a <unk> even central banks **</s>**\n",
            "speculators are calling for a degree of liquidity that is **a**\n",
            "many money managers and some traders had already left their **<unk>**\n",
            "then in a <unk> plunge the dow jones industrials in **the**\n",
            "<unk> trading accelerated to N million shares a record for **the**\n",
            "at the end of the day N million shares were **n't**\n",
            "the dow 's decline was second in point terms only **<num>**\n",
            "in percentage terms however the dow 's dive was the **<unk>**\n",
            "shares of ual the parent of united airlines were extremely **high**\n",
            "wall street 's takeover-stock speculators or risk arbitragers had placed **convertible**\n",
            "at N p.m. edt came the <unk> news the big **board**\n",
            "on the exchange floor as soon as ual stopped trading **</s>**\n",
            "several traders could be seen shaking their heads when the **<unk>**\n",
            "for weeks the market had been nervous about takeovers after **the**\n",
            "and N minutes after the ual trading halt came news **that**\n",
            "arbitragers could n't dump their ual stock but they rid **<unk>**\n",
            "for example their selling caused trading halts to be declared **a**\n",
            "but as panic spread speculators began to sell blue-chip stocks **</s>**\n",
            "when trading was halted in philip morris the stock was **<unk>**\n",
            "selling <unk> because of waves of automatic stop-loss orders which **has**\n",
            "most of the stock selling pressure came from wall street **'s**\n",
            "traders said most of their major institutional investors on the **<unk>**\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEfAMbGIL0bL"
      },
      "source": [
        "It appears that the MLE and Good Turing predictions are very similar, if not completely the same. For the most part, I think that the N-gram language modeling worked decently for these sequences, although there are for sure a few mistakes, and a significant amount of < unk > and < / s> predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Ss79ThejMo0U",
        "outputId": "770d32fa-82f8-4448-94b6-16b12a275a4c"
      },
      "source": [
        "'''\n",
        "# Kneser-Ney N-gram - Print the 30 sequences and their next words (specified in ** **)\n",
        "print('KNESER-NEY PREDICTIONS\\n')\n",
        "for i in range(30):\n",
        "  # Get prediction\n",
        "  pred = ngram_predict(input_sequences_30[i], pkn)\n",
        "\n",
        "  # Print completed sequence\n",
        "  print(input_sequences_30[i] + ' ' + '**' + pred + '**')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Kneser-Ney N-gram - Print the 30 sequences and their next words (specified in ** **)\\nprint('KNESER-NEY PREDICTIONS\\n')\\nfor i in range(30):\\n  # Get prediction\\n  pred = ngram_predict(input_sequences_30[i], pkn)\\n\\n  # Print completed sequence\\n  print(input_sequences_30[i] + ' ' + '**' + pred + '**')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 490
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjcgrrN_2Xmc"
      },
      "source": [
        "# **2. RNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmFDLDKb2bM3"
      },
      "source": [
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTMCell, Dense, GRU, Embedding\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfmGMu8T3xEb"
      },
      "source": [
        "## <font color=navy> **Parameter Initialization (2a)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abJvA8Xt3WT7"
      },
      "source": [
        "# Parameters (used in 2d)\n",
        "learning_rate = 1E-3\n",
        "window_size = 20\n",
        "batch_size = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3sXx786Q13r"
      },
      "source": [
        "## <font color=navy> **Further Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOUyn9mLp5Y3"
      },
      "source": [
        "# Remove <s> and </s> tokens from data and vocabulary (take second to last index because empty)\n",
        "train_clean = [t[4:-5] for t in train][:-1]\n",
        "\n",
        "# Update vocabulary\n",
        "vocab_clean = vocabulary.copy()\n",
        "vocab_clean.remove('<s>')\n",
        "vocab_clean.remove('</s>')\n",
        "\n",
        "# Encode the sequences using Keras integer mapping\n",
        "train_encoded = [one_hot(s, len(vocab_clean)) for s in train_clean]\n",
        "\n",
        "# Create X and y (last word in sequence)\n",
        "X_train = [x[:-1] for x in train_encoded]\n",
        "y_train = [x[-1] - 1 for x in train_encoded]\n",
        "\n",
        "# Pad X_train\n",
        "X_train = pad_sequences(X_train, maxlen=window_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEP2mLZlRcZ2",
        "outputId": "137a5e7b-e950-4f48-c0c9-898b8ab7e6a0"
      },
      "source": [
        "# Confirm\n",
        "print(len(train_encoded))\n",
        "print(X_train.shape)\n",
        "print(len(y_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n",
            "(5000, 20)\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39GHUHl45HiA"
      },
      "source": [
        "## <font color=navy> **Forward Pass (2b)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0L6AWLra5Gs-",
        "outputId": "64b026b9-bc58-47ed-a12c-4cf41e3ebc6e"
      },
      "source": [
        "# Define model architecture\n",
        "# Specify Sequential model\n",
        "rnn = Sequential()\n",
        "\n",
        "# Add Embedding (input) layer\n",
        "# - input_dim = size of vocabulary\n",
        "# - output_dim = batch_size\n",
        "# - input length = window_size\n",
        "rnn.add(Embedding(input_dim = len(vocab_clean), output_dim = batch_size, input_length=window_size))\n",
        "\n",
        "# Add recurrent neural network cell (LSTM because good default choice)\n",
        "rnn.add(LSTM(150))\n",
        "\n",
        "# Add Dense layer \n",
        "rnn.add(Dense(len(vocab_clean), activation='softmax'))\n",
        "\n",
        "# Print model summary\n",
        "rnn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_29 (Embedding)     (None, 20, 50)            359250    \n",
            "_________________________________________________________________\n",
            "lstm_27 (LSTM)               (None, 150)               120600    \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 7185)              1084935   \n",
            "=================================================================\n",
            "Total params: 1,564,785\n",
            "Trainable params: 1,564,785\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW5FjzphZB3j"
      },
      "source": [
        "# Define prediction function for next word\n",
        "def predict_next_word(softmax_output, vocab):\n",
        "  # Encode vocabulary\n",
        "  vocab_encoded = [one_hot(v, len(vocab)) for v in vocab]\n",
        "\n",
        "  # Get index of maximum input\n",
        "  ind = np.argmax(softmax_output, axis=1)\n",
        "\n",
        "  # Return encoded prediction vector and word prediction vector\n",
        "  return ind, [vocab[i] for i in ind] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfz7dCMO-0Z-"
      },
      "source": [
        "# Implement forward pass\n",
        "forward_pass = rnn(X_train)\n",
        "\n",
        "# Predict next word for all of the sequences\n",
        "ind, word_pred = predict_next_word(forward_pass, vocab_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L60yphBEZ_ux",
        "outputId": "8044a0cf-6d57-4b60-a0e3-e5bf219bf0d1"
      },
      "source": [
        "# Print predictions shape and first example of sequence + prediction to confirm\n",
        "print(len(word_pred))\n",
        "print()\n",
        "print('EXAMPLE SEQUENCE PREDICTION:')\n",
        "print('Predicted sequence: ' + X[1] + '[' + word_pred[1] + ']')\n",
        "print('\\nActual sequence: ' + X[1] + '[' + y[1] + ']')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n",
            "\n",
            "EXAMPLE SEQUENCE PREDICTION:\n",
            "Predicted sequence: pierre <unk> n years old will join the board as a nonexecutive director nov.[peddling]\n",
            "\n",
            "Actual sequence: pierre <unk> n years old will join the board as a nonexecutive director nov.[n]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h17CmwYY-N3J"
      },
      "source": [
        "## <font color=navy> **Calculate model loss (2c)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BC_cSaqcHrK",
        "outputId": "6c9a5bcb-99d9-4cd0-c96d-05354e08cab3"
      },
      "source": [
        "# Calculate sparse categorical cross entropy loss\n",
        "initial_loss = SparseCategoricalCrossentropy()(y_train, forward_pass)\n",
        "print(initial_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(8.879803, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeP7bUMw-Taz"
      },
      "source": [
        "## <font color=navy> **Setup training step (2d)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoNbWX2Uh_fh"
      },
      "source": [
        "# Compile model\n",
        "rnn.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hl1Bt7i-Y-O"
      },
      "source": [
        "## <font color=navy> **Train RNN using training set (2e)**\n",
        "* Trained for 100 epochs\n",
        "* Used 10% of data for validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5cO3TltwhIu",
        "outputId": "63b3bf85-0ab3-4979-cdc4-96aa594833d9"
      },
      "source": [
        "# Train/validation split\n",
        "X_tr = X_train[:4500]\n",
        "y_tr = np.asarray(y_train[:4500]).astype('float32')\n",
        "X_val = X_train[4500:]\n",
        "y_val = np.asarray(y_train[4500:]).astype('float32')\n",
        "\n",
        "# Confirm shapes\n",
        "print(X_tr.shape)\n",
        "print(y_tr.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4500, 20)\n",
            "(4500,)\n",
            "(500, 20)\n",
            "(500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEN3vEvji_8Z",
        "outputId": "7b0ee73f-f884-4fac-9d9a-4da8e6d615ec"
      },
      "source": [
        "# Train RNN\n",
        "history = rnn.fit(X_tr, y_tr, batch_size=batch_size, epochs=100, validation_data=(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "90/90 [==============================] - 7s 57ms/step - loss: 8.1891 - val_loss: 7.2873\n",
            "Epoch 2/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 6.5534 - val_loss: 7.4023\n",
            "Epoch 3/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 6.2948 - val_loss: 7.4303\n",
            "Epoch 4/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 6.2041 - val_loss: 7.5456\n",
            "Epoch 5/100\n",
            "90/90 [==============================] - 5s 51ms/step - loss: 6.1897 - val_loss: 7.5703\n",
            "Epoch 6/100\n",
            "90/90 [==============================] - 5s 51ms/step - loss: 6.0669 - val_loss: 7.6191\n",
            "Epoch 7/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 5.9447 - val_loss: 7.7048\n",
            "Epoch 8/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 5.8907 - val_loss: 7.7752\n",
            "Epoch 9/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 5.7725 - val_loss: 7.7974\n",
            "Epoch 10/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 5.6970 - val_loss: 7.8623\n",
            "Epoch 11/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 5.5937 - val_loss: 7.9333\n",
            "Epoch 12/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 5.4974 - val_loss: 7.9946\n",
            "Epoch 13/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 5.3585 - val_loss: 8.0984\n",
            "Epoch 14/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 5.2372 - val_loss: 8.1243\n",
            "Epoch 15/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 5.1339 - val_loss: 8.3291\n",
            "Epoch 16/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 5.0513 - val_loss: 8.3832\n",
            "Epoch 17/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 4.9472 - val_loss: 8.4595\n",
            "Epoch 18/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 4.8292 - val_loss: 8.6303\n",
            "Epoch 19/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 5.0539 - val_loss: 8.5596\n",
            "Epoch 20/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 4.7654 - val_loss: 8.6620\n",
            "Epoch 21/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 4.5732 - val_loss: 8.7699\n",
            "Epoch 22/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 4.5661 - val_loss: 8.7819\n",
            "Epoch 23/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 4.4187 - val_loss: 8.8577\n",
            "Epoch 24/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 4.3371 - val_loss: 8.9654\n",
            "Epoch 25/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 4.2528 - val_loss: 9.0437\n",
            "Epoch 26/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 4.1467 - val_loss: 9.1198\n",
            "Epoch 27/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 4.0723 - val_loss: 9.2031\n",
            "Epoch 28/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 3.9973 - val_loss: 9.2767\n",
            "Epoch 29/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 3.9249 - val_loss: 9.3617\n",
            "Epoch 30/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 3.8232 - val_loss: 9.4362\n",
            "Epoch 31/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 3.6875 - val_loss: 9.5125\n",
            "Epoch 32/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 3.6737 - val_loss: 9.6079\n",
            "Epoch 33/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 3.5176 - val_loss: 9.6700\n",
            "Epoch 34/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 3.4944 - val_loss: 9.7529\n",
            "Epoch 35/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 3.3748 - val_loss: 9.8202\n",
            "Epoch 36/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 3.2555 - val_loss: 9.9080\n",
            "Epoch 37/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 3.2287 - val_loss: 9.9974\n",
            "Epoch 38/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 3.1841 - val_loss: 10.0788\n",
            "Epoch 39/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 3.0576 - val_loss: 10.1525\n",
            "Epoch 40/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 2.9968 - val_loss: 10.2271\n",
            "Epoch 41/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 2.9364 - val_loss: 10.3020\n",
            "Epoch 42/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 2.8180 - val_loss: 10.4000\n",
            "Epoch 43/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 2.7382 - val_loss: 10.4692\n",
            "Epoch 44/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 2.6542 - val_loss: 10.5566\n",
            "Epoch 45/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 2.6049 - val_loss: 10.6230\n",
            "Epoch 46/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 2.5348 - val_loss: 10.7130\n",
            "Epoch 47/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 2.4739 - val_loss: 10.7676\n",
            "Epoch 48/100\n",
            "90/90 [==============================] - 5s 52ms/step - loss: 2.3465 - val_loss: 10.8978\n",
            "Epoch 49/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 2.2999 - val_loss: 10.9562\n",
            "Epoch 50/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 2.2425 - val_loss: 11.0038\n",
            "Epoch 51/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 2.1777 - val_loss: 11.1018\n",
            "Epoch 52/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 2.1329 - val_loss: 11.1687\n",
            "Epoch 53/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 2.0530 - val_loss: 11.2388\n",
            "Epoch 54/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 1.9868 - val_loss: 11.3213\n",
            "Epoch 55/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 1.9081 - val_loss: 11.3532\n",
            "Epoch 56/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 1.8200 - val_loss: 11.4510\n",
            "Epoch 57/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 1.7691 - val_loss: 11.5222\n",
            "Epoch 58/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 1.7169 - val_loss: 11.5995\n",
            "Epoch 59/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 1.6736 - val_loss: 11.6667\n",
            "Epoch 60/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 1.6047 - val_loss: 11.7284\n",
            "Epoch 61/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 1.5323 - val_loss: 11.8297\n",
            "Epoch 62/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 1.5835 - val_loss: 11.8552\n",
            "Epoch 63/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 1.5445 - val_loss: 11.9515\n",
            "Epoch 64/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 1.4591 - val_loss: 11.9722\n",
            "Epoch 65/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 1.3800 - val_loss: 12.0384\n",
            "Epoch 66/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 1.3169 - val_loss: 12.1219\n",
            "Epoch 67/100\n",
            "90/90 [==============================] - 5s 58ms/step - loss: 1.2931 - val_loss: 12.1833\n",
            "Epoch 68/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 1.2400 - val_loss: 12.2589\n",
            "Epoch 69/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 1.2023 - val_loss: 12.2916\n",
            "Epoch 70/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 1.1177 - val_loss: 12.3652\n",
            "Epoch 71/100\n",
            "90/90 [==============================] - 5s 55ms/step - loss: 1.1271 - val_loss: 12.4061\n",
            "Epoch 72/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 1.0672 - val_loss: 12.4762\n",
            "Epoch 73/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 1.0651 - val_loss: 12.5552\n",
            "Epoch 74/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 1.0372 - val_loss: 12.6241\n",
            "Epoch 75/100\n",
            "90/90 [==============================] - 5s 55ms/step - loss: 0.9734 - val_loss: 12.6691\n",
            "Epoch 76/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 0.9263 - val_loss: 12.7335\n",
            "Epoch 77/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 0.8987 - val_loss: 12.8029\n",
            "Epoch 78/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 0.8667 - val_loss: 12.8839\n",
            "Epoch 79/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 0.8464 - val_loss: 12.9398\n",
            "Epoch 80/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 0.7972 - val_loss: 12.9438\n",
            "Epoch 81/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 0.8028 - val_loss: 13.0619\n",
            "Epoch 82/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 0.7695 - val_loss: 13.1041\n",
            "Epoch 83/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 0.7078 - val_loss: 13.1981\n",
            "Epoch 84/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 0.7002 - val_loss: 13.2418\n",
            "Epoch 85/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 0.6741 - val_loss: 13.2946\n",
            "Epoch 86/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 0.6542 - val_loss: 13.3709\n",
            "Epoch 87/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 0.6295 - val_loss: 13.4113\n",
            "Epoch 88/100\n",
            "90/90 [==============================] - 5s 53ms/step - loss: 0.6248 - val_loss: 13.4818\n",
            "Epoch 89/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 0.5783 - val_loss: 13.5643\n",
            "Epoch 90/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 0.5697 - val_loss: 13.6347\n",
            "Epoch 91/100\n",
            "90/90 [==============================] - 5s 55ms/step - loss: 0.5614 - val_loss: 13.6746\n",
            "Epoch 92/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 0.5292 - val_loss: 13.7839\n",
            "Epoch 93/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 0.4940 - val_loss: 13.8207\n",
            "Epoch 94/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 0.4843 - val_loss: 13.8506\n",
            "Epoch 95/100\n",
            "90/90 [==============================] - 5s 55ms/step - loss: 0.4719 - val_loss: 13.9116\n",
            "Epoch 96/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 0.4398 - val_loss: 14.0063\n",
            "Epoch 97/100\n",
            "90/90 [==============================] - 5s 55ms/step - loss: 0.4364 - val_loss: 14.0181\n",
            "Epoch 98/100\n",
            "90/90 [==============================] - 5s 55ms/step - loss: 0.4401 - val_loss: 14.1194\n",
            "Epoch 99/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 0.4128 - val_loss: 14.1661\n",
            "Epoch 100/100\n",
            "90/90 [==============================] - 5s 54ms/step - loss: 0.3937 - val_loss: 14.2285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCUJswO40pAK"
      },
      "source": [
        "Based on the validation loss, the model is overfitting within the first epoch. Nonetheless, I trained the model for 100 epochs to minimize the training less (although that is not at all good practice)!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU4tBvC_-izt"
      },
      "source": [
        "## <font color=navy> **Test RNN model / Calculate perplexity (2f)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEu7T3Mn1Dv7"
      },
      "source": [
        "# Preprocess validation data\n",
        "# Remove <s> and </s> tokens from data\n",
        "test_clean = [v[4:-5] for v in validation][:-1]\n",
        "\n",
        "# Encode the sequences using Keras integer mapping\n",
        "test_encoded = [one_hot(val, len(vocab_clean)) for val in test_clean]\n",
        "\n",
        "# Create X and y (last word in sequence)\n",
        "X_test = [x[:-1] for x in test_encoded]\n",
        "y_test = [x[-1] - 1 for x in test_encoded]\n",
        "\n",
        "# Pad X_test\n",
        "X_test = pad_sequences(X_test, maxlen=window_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd_eT-Tl150v",
        "outputId": "484521f6-a82a-494f-a460-9fb01095de90"
      },
      "source": [
        "# Confirm\n",
        "print(len(test_encoded))\n",
        "print(X_test.shape)\n",
        "print(len(y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3370\n",
            "(3370, 20)\n",
            "3370\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pIskdRX2CMO"
      },
      "source": [
        "# Get predictions\n",
        "test_softmax = rnn(X_test)\n",
        "\n",
        "# Get test prediction index vector\n",
        "test_ind_vec, test_word_vec = predict_next_word(test_softmax, vocab_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9BvSBGS7PcG",
        "outputId": "fd9830af-30af-44f1-a514-d9d0a3d7b790"
      },
      "source": [
        "# Calculate model perplexity on test set\n",
        "test_perplexity = 0\n",
        "\n",
        "# Convert test_softmax to array\n",
        "test_softmax = np.array(test_softmax)\n",
        "\n",
        "for i in range(len(test_softmax)):\n",
        "  # Get correct probability\n",
        "  correct_prob = test_softmax[0][y_test[i]]\n",
        "\n",
        "  # Add -log of correct_prob to test_perplexity\n",
        "  test_perplexity += -np.log(correct_prob)\n",
        "\n",
        "# Average and report\n",
        "print('Test perplexity: ', np.exp(test_perplexity / len(test_softmax)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test perplexity:  47102243.3032223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfSHSYuP_gtx"
      },
      "source": [
        "That number seems very high!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwrkghCt-pgn"
      },
      "source": [
        "## <font color=navy> **Perplexity proof (2g)**\n",
        "* See other submission, 2g_Aprile_Allison.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J60IzD9-ukl"
      },
      "source": [
        "## <font color=navy> **Print input_sequence next word (2h)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs5jj72y-b0c"
      },
      "source": [
        "# Preprocess input_sequence data\n",
        "\n",
        "# Encode the sequences using Keras integer mapping\n",
        "input_encoded = [one_hot(seq, len(vocab_clean)) for seq in input_sequences_30]\n",
        "\n",
        "# Pad encoded sequences\n",
        "X_input = pad_sequences(input_encoded, maxlen=window_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "564m2OiLEAXH",
        "outputId": "e8cf6726-9361-48d1-da76-bbe01c1f2260"
      },
      "source": [
        "# Confirm shapes\n",
        "print(len(input_encoded))\n",
        "print(X_input.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30\n",
            "(30, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvFEakqCENeK"
      },
      "source": [
        "# Get probability predictions for next word\n",
        "input_preds = rnn(X_input)\n",
        "\n",
        "# Get test prediction index vector\n",
        "input_ind_vec, input_word_vec = predict_next_word(input_preds, vocab_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KauDqEcOEqDo",
        "outputId": "865595e3-fcc9-4f1f-cac5-4f0cddcd8b41"
      },
      "source": [
        "# Print the 30 sequences and their next words (specified in ** **)\n",
        "for i in range(30):\n",
        "  print(input_sequences_30[i] + ' ' + '**' + input_word_vec[i] + '**')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "but while the new york stock exchange did n't fall **brushed**\n",
            "some circuit breakers installed after the october N crash failed **eurodollars**\n",
            "the N stock specialist firms on the big board floor **find**\n",
            "big investment banks refused to step up to the plate **aspect**\n",
            "heavy selling of standard & poor 's 500-stock index futures **damage**\n",
            "seven big board stocks ual amr bankamerica walt disney capital **sleep**\n",
            "once again the specialists were not able to handle the **publishers**\n",
            "<unk> james <unk> chairman of specialists henderson brothers inc. it **'s**\n",
            "when the dollar is in a <unk> even central banks **sentence**\n",
            "speculators are calling for a degree of liquidity that is **mortgages**\n",
            "many money managers and some traders had already left their **chemicals**\n",
            "then in a <unk> plunge the dow jones industrials in **minneapolis**\n",
            "<unk> trading accelerated to N million shares a record for **peddling**\n",
            "at the end of the day N million shares were **market-share**\n",
            "the dow 's decline was second in point terms only **sleep**\n",
            "in percentage terms however the dow 's dive was the **tip**\n",
            "shares of ual the parent of united airlines were extremely **partly**\n",
            "wall street 's takeover-stock speculators or risk arbitragers had placed **sponsor**\n",
            "at N p.m. edt came the <unk> news the big **sleep**\n",
            "on the exchange floor as soon as ual stopped trading **venture**\n",
            "several traders could be seen shaking their heads when the **chemicals**\n",
            "for weeks the market had been nervous about takeovers after **newly**\n",
            "and N minutes after the ual trading halt came news **ag**\n",
            "arbitragers could n't dump their ual stock but they rid **priority**\n",
            "for example their selling caused trading halts to be declared **center**\n",
            "but as panic spread speculators began to sell blue-chip stocks **allies**\n",
            "when trading was halted in philip morris the stock was **upside**\n",
            "selling <unk> because of waves of automatic stop-loss orders which **income**\n",
            "most of the stock selling pressure came from wall street **mullins**\n",
            "traders said most of their major institutional investors on the **competitors**\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRGQp5wAFXyV"
      },
      "source": [
        "Some of the above sequences are completed with reasonable words, although I feel that most of them are not logical. To improve this, I could do more text processing and add more layers/regularization to my model, as well as try out other embeddings with better bidirectionality (such as BERT), in the future."
      ]
    }
  ]
}